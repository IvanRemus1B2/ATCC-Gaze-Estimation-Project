For Simple-1-1(128, 128):
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 image (InputLayer)             [(None, 128, 128, 3  0           []
                                )]

 conv2d (Conv2D)                (None, 128, 128, 64  9472        ['image[0][0]']
                                )

 batch_normalization (BatchNorm  (None, 128, 128, 64  256        ['conv2d[0][0]']
 alization)                     )

 conv2d_1 (Conv2D)              (None, 128, 128, 12  204928      ['batch_normalization[0][0]']
                                8)

 batch_normalization_1 (BatchNo  (None, 128, 128, 12  512        ['conv2d_1[0][0]']
 rmalization)                   8)

 max_pooling2d (MaxPooling2D)   (None, 42, 42, 128)  0           ['batch_normalization_1[0][0]']

 conv2d_2 (Conv2D)              (None, 42, 42, 64)   204864      ['max_pooling2d[0][0]']

 batch_normalization_2 (BatchNo  (None, 42, 42, 64)  256         ['conv2d_2[0][0]']
 rmalization)

 conv2d_3 (Conv2D)              (None, 42, 42, 32)   18464       ['batch_normalization_2[0][0]']

 batch_normalization_3 (BatchNo  (None, 42, 42, 32)  128         ['conv2d_3[0][0]']
 rmalization)

 max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 32)  0           ['batch_normalization_3[0][0]']

 flatten (Flatten)              (None, 6272)         0           ['max_pooling2d_1[0][0]']

 info (InputLayer)              [(None, 5)]          0           []

 concatenate (Concatenate)      (None, 6277)         0           ['flatten[0][0]',
                                                                  'info[0][0]']

 dense (Dense)                  (None, 256)          1607168     ['concatenate[0][0]']

 batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense[0][0]']
 rmalization)

 dense_1 (Dense)                (None, 128)          32896       ['batch_normalization_4[0][0]']

 batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_1[0][0]']
 rmalization)

 dense_2 (Dense)                (None, 64)           8256        ['batch_normalization_5[0][0]']

 batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_2[0][0]']
 rmalization)

 pixel_prediction (Dense)       (None, 2)            130         ['batch_normalization_6[0][0]']

==================================================================================================
Total params: 2,089,122
Trainable params: 2,087,650
Non-trainable params: 1,472
__________________________________________________________________________________________________
Epoch 1/5
2024-05-25 22:57:11.274556: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2024-05-25 22:57:17.787489: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 22:57:17.788376: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
233/266 [=========================>....] - ETA: 38s - loss: 0.6117 - mean_squared_error: 0.61172024-05-25 23:01:47.372906: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:01:47.373657: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:01:47.550082: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:01:47.550807: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
266/266 [==============================] - ETA: 0s - loss: 0.5724 - mean_squared_error: 0.5724
Epoch 1: val_loss improved from inf to 20.48437, saving model to Models\Simple-1-(128, 128).h5
266/266 [==============================] - 347s 1s/step - loss: 0.5724 - mean_squared_error: 0.5724 - val_loss: 20.4844 - val_mean_squared_error: 20.4844
Epoch 2/5
266/266 [==============================] - ETA: 0s - loss: 0.1801 - mean_squared_error: 0.1801
Epoch 2: val_loss improved from 20.48437 to 4.80259, saving model to Models\Simple-1-(128, 128).h5
266/266 [==============================] - 334s 1s/step - loss: 0.1801 - mean_squared_error: 0.1801 - val_loss: 4.8026 - val_mean_squared_error: 4.8026
Epoch 3/5
266/266 [==============================] - ETA: 0s - loss: 0.1087 - mean_squared_error: 0.1087
Epoch 3: val_loss did not improve from 4.80259
266/266 [==============================] - 356s 1s/step - loss: 0.1087 - mean_squared_error: 0.1087 - val_loss: 9.8065 - val_mean_squared_error: 9.8065
Epoch 4/5
266/266 [==============================] - ETA: 0s - loss: 0.0867 - mean_squared_error: 0.0867
Epoch 4: val_loss did not improve from 4.80259
266/266 [==============================] - 388s 1s/step - loss: 0.0867 - mean_squared_error: 0.0867 - val_loss: 6.0587 - val_mean_squared_error: 6.0587
Epoch 5/5
266/266 [==============================] - ETA: 0s - loss: 0.0748 - mean_squared_error: 0.0748
Epoch 5: val_loss did not improve from 4.80259
266/266 [==============================] - 359s 1s/step - loss: 0.0748 - mean_squared_error: 0.0748 - val_loss: 9.8690 - val_mean_squared_error: 9.8690

For Simple-1-(128, 128):
For pog corrected train3.csv mse_loss: 3.7427
For pog corrected validation3.csv mse_loss: 4.8026
For pog corrected test3.csv mse_loss: 4.9070

------------------------------
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 image (InputLayer)             [(None, 128, 128, 3  0           []
                                )]

 gaussian_noise (GaussianNoise)  (None, 128, 128, 3)  0          ['image[0][0]']

 conv2d (Conv2D)                (None, 128, 128, 64  9472        ['gaussian_noise[0][0]']
                                )

 batch_normalization (BatchNorm  (None, 128, 128, 64  256        ['conv2d[0][0]']
 alization)                     )

 conv2d_1 (Conv2D)              (None, 128, 128, 12  204928      ['batch_normalization[0][0]']
                                8)

 batch_normalization_1 (BatchNo  (None, 128, 128, 12  512        ['conv2d_1[0][0]']
 rmalization)                   8)

 max_pooling2d (MaxPooling2D)   (None, 42, 42, 128)  0           ['batch_normalization_1[0][0]']

 conv2d_2 (Conv2D)              (None, 42, 42, 64)   204864      ['max_pooling2d[0][0]']

 batch_normalization_2 (BatchNo  (None, 42, 42, 64)  256         ['conv2d_2[0][0]']
 rmalization)

 conv2d_3 (Conv2D)              (None, 42, 42, 32)   18464       ['batch_normalization_2[0][0]']

 batch_normalization_3 (BatchNo  (None, 42, 42, 32)  128         ['conv2d_3[0][0]']
 rmalization)

 max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 32)  0           ['batch_normalization_3[0][0]']

 flatten (Flatten)              (None, 6272)         0           ['max_pooling2d_1[0][0]']

 info (InputLayer)              [(None, 5)]          0           []

 concatenate (Concatenate)      (None, 6277)         0           ['flatten[0][0]',
                                                                  'info[0][0]']

 dense (Dense)                  (None, 256)          1607168     ['concatenate[0][0]']

 batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense[0][0]']
 rmalization)

 dense_1 (Dense)                (None, 128)          32896       ['batch_normalization_4[0][0]']

 batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_1[0][0]']
 rmalization)

 dense_2 (Dense)                (None, 64)           8256        ['batch_normalization_5[0][0]']

 batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_2[0][0]']
 rmalization)

 pixel_prediction (Dense)       (None, 2)            130         ['batch_normalization_6[0][0]']

==================================================================================================
Total params: 2,089,122
Trainable params: 2,087,650
Non-trainable params: 1,472
__________________________________________________________________________________________________
2024-05-25 23:50:14.163606: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
 94/266 [=========>....................] - ETA: 3:34 - loss: 0.2857 - mean_squared_error: 0.28572024-05-25 23:52:17.171792: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:52:17.172439: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:52:17.573435: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:52:17.574100: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:52:17.757647: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:52:17.758347: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
266/266 [==============================] - ETA: 0s - loss: 0.1602 - mean_squared_error: 0.16022024-05-25 23:56:24.865981: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.90GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:56:24.866699: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.90GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:56:24.867378: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.62GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-25 23:56:24.868064: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.62GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.

Epoch 1: val_loss improved from inf to 0.13881, saving model to Models\Simple-2-(128, 128).h5
266/266 [==============================] - 374s 1s/step - loss: 0.1602 - mean_squared_error: 0.1602 - val_loss: 0.1388 - val_mean_squared_error: 0.1388

For Simple-2-(128, 128):
For pog corrected train3.csv mse_loss: 0.1327
For pog corrected validation3.csv mse_loss: 0.1388
For pog corrected test3.csv mse_loss: 0.1439


For Simple-4-(128, 128):
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 image (InputLayer)             [(None, 128, 128, 3  0           []
                                )]

 gaussian_noise (GaussianNoise)  (None, 128, 128, 3)  0          ['image[0][0]']

 conv2d (Conv2D)                (None, 128, 128, 64  9472        ['gaussian_noise[0][0]']
                                )

 batch_normalization (BatchNorm  (None, 128, 128, 64  256        ['conv2d[0][0]']
 alization)                     )

 conv2d_1 (Conv2D)              (None, 128, 128, 12  204928      ['batch_normalization[0][0]']
                                8)

 batch_normalization_1 (BatchNo  (None, 128, 128, 12  512        ['conv2d_1[0][0]']
 rmalization)                   8)

 max_pooling2d (MaxPooling2D)   (None, 42, 42, 128)  0           ['batch_normalization_1[0][0]']

 conv2d_2 (Conv2D)              (None, 42, 42, 64)   204864      ['max_pooling2d[0][0]']

 batch_normalization_2 (BatchNo  (None, 42, 42, 64)  256         ['conv2d_2[0][0]']
 rmalization)

 conv2d_3 (Conv2D)              (None, 42, 42, 32)   18464       ['batch_normalization_2[0][0]']

 batch_normalization_3 (BatchNo  (None, 42, 42, 32)  128         ['conv2d_3[0][0]']
 rmalization)

 max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 32)  0           ['batch_normalization_3[0][0]']

 dropout (Dropout)              (None, 14, 14, 32)   0           ['max_pooling2d_1[0][0]']

 flatten (Flatten)              (None, 6272)         0           ['dropout[0][0]']

 info (InputLayer)              [(None, 5)]          0           []

 concatenate (Concatenate)      (None, 6277)         0           ['flatten[0][0]',
                                                                  'info[0][0]']

 dense (Dense)                  (None, 256)          1607168     ['concatenate[0][0]']

 batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense[0][0]']
 rmalization)

 dense_1 (Dense)                (None, 128)          32896       ['batch_normalization_4[0][0]']

 batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_1[0][0]']
 rmalization)

 dense_2 (Dense)                (None, 64)           8256        ['batch_normalization_5[0][0]']

 batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_2[0][0]']
 rmalization)

 pixel_prediction (Dense)       (None, 2)            130         ['batch_normalization_6[0][0]']

==================================================================================================
Total params: 2,089,122
Trainable params: 2,087,650
Non-trainable params: 1,472
__________________________________________________________________________________________________
Epoch 1/100
2024-05-26 01:19:48.850842: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
203/266 [=====================>........] - ETA: 1:28 - loss: 2.6579 - mean_absolute_error: 0.31472024-05-26 01:24:40.053211: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:24:40.053968: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:24:40.488094: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:24:40.489052: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:24:40.668542: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:24:40.670056: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
266/266 [==============================] - ETA: 0s - loss: 2.1557 - mean_absolute_error: 0.30152024-05-26 01:26:32.990368: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.90GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:26:32.991137: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.90GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:26:32.991852: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.62GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
2024-05-26 01:26:32.992478: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.62GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.

Epoch 1: val_mean_absolute_error improved from inf to 0.34782, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 408s 2s/step - loss: 2.1557 - mean_absolute_error: 0.3015 - val_loss: 0.4396 - val_mean_absolute_error: 0.3478
Epoch 2/100
266/266 [==============================] - ETA: 0s - loss: 0.1789 - mean_absolute_error: 0.2512
Epoch 2: val_mean_absolute_error improved from 0.34782 to 0.32267, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 325s 1s/step - loss: 0.1789 - mean_absolute_error: 0.2512 - val_loss: 0.1147 - val_mean_absolute_error: 0.3227
Epoch 3/100
266/266 [==============================] - ETA: 0s - loss: 0.0583 - mean_absolute_error: 0.2452
Epoch 3: val_mean_absolute_error did not improve from 0.32267
266/266 [==============================] - 310s 1s/step - loss: 0.0583 - mean_absolute_error: 0.2452 - val_loss: 0.1077 - val_mean_absolute_error: 0.3625
Epoch 4/100
266/266 [==============================] - ETA: 0s - loss: 0.0473 - mean_absolute_error: 0.2349
Epoch 4: val_mean_absolute_error did not improve from 0.32267
266/266 [==============================] - 310s 1s/step - loss: 0.0473 - mean_absolute_error: 0.2349 - val_loss: 0.2205 - val_mean_absolute_error: 0.5438
Epoch 5/100
266/266 [==============================] - ETA: 0s - loss: 0.0448 - mean_absolute_error: 0.2218
Epoch 5: val_mean_absolute_error improved from 0.32267 to 0.27550, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 311s 1s/step - loss: 0.0448 - mean_absolute_error: 0.2218 - val_loss: 0.0656 - val_mean_absolute_error: 0.2755
Epoch 6/100
266/266 [==============================] - ETA: 0s - loss: 0.0401 - mean_absolute_error: 0.2012
Epoch 6: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 310s 1s/step - loss: 0.0401 - mean_absolute_error: 0.2012 - val_loss: 0.1410 - val_mean_absolute_error: 0.4235
Epoch 7/100
266/266 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.1846
Epoch 7: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 314s 1s/step - loss: 0.0359 - mean_absolute_error: 0.1846 - val_loss: 0.1643 - val_mean_absolute_error: 0.4680
Epoch 8/100
266/266 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.1688
Epoch 8: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 312s 1s/step - loss: 0.0315 - mean_absolute_error: 0.1688 - val_loss: 0.0871 - val_mean_absolute_error: 0.3164
Epoch 9/100
266/266 [==============================] - ETA: 0s - loss: 0.0284 - mean_absolute_error: 0.1578
Epoch 9: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 312s 1s/step - loss: 0.0284 - mean_absolute_error: 0.1578 - val_loss: 0.1635 - val_mean_absolute_error: 0.4408
Epoch 10/100
266/266 [==============================] - ETA: 0s - loss: 0.0260 - mean_absolute_error: 0.1504
Epoch 10: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 311s 1s/step - loss: 0.0260 - mean_absolute_error: 0.1504 - val_loss: 0.1292 - val_mean_absolute_error: 0.3737
Epoch 11/100
266/266 [==============================] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.1456
Epoch 11: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 312s 1s/step - loss: 0.0245 - mean_absolute_error: 0.1456 - val_loss: 0.1028 - val_mean_absolute_error: 0.3460
Epoch 12/100
266/266 [==============================] - ETA: 0s - loss: 0.0239 - mean_absolute_error: 0.1410
Epoch 12: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 315s 1s/step - loss: 0.0239 - mean_absolute_error: 0.1410 - val_loss: 0.1893 - val_mean_absolute_error: 0.5004
Epoch 13/100
266/266 [==============================] - ETA: 0s - loss: 0.0220 - mean_absolute_error: 0.1351
Epoch 13: val_mean_absolute_error did not improve from 0.27550
266/266 [==============================] - 311s 1s/step - loss: 0.0220 - mean_absolute_error: 0.1351 - val_loss: 0.1788 - val_mean_absolute_error: 0.5328
Epoch 14/100
266/266 [==============================] - ETA: 0s - loss: 0.0209 - mean_absolute_error: 0.1322
Epoch 14: val_mean_absolute_error improved from 0.27550 to 0.26926, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 312s 1s/step - loss: 0.0209 - mean_absolute_error: 0.1322 - val_loss: 0.0672 - val_mean_absolute_error: 0.2693
Epoch 15/100
266/266 [==============================] - ETA: 0s - loss: 0.0205 - mean_absolute_error: 0.1291
Epoch 15: val_mean_absolute_error improved from 0.26926 to 0.24838, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 313s 1s/step - loss: 0.0205 - mean_absolute_error: 0.1291 - val_loss: 0.0627 - val_mean_absolute_error: 0.2484
Epoch 16/100
266/266 [==============================] - ETA: 0s - loss: 0.0199 - mean_absolute_error: 0.1280
Epoch 16: val_mean_absolute_error did not improve from 0.24838
266/266 [==============================] - 312s 1s/step - loss: 0.0199 - mean_absolute_error: 0.1280 - val_loss: 0.1409 - val_mean_absolute_error: 0.4413
Epoch 17/100
266/266 [==============================] - ETA: 0s - loss: 0.0188 - mean_absolute_error: 0.1244
Epoch 17: val_mean_absolute_error did not improve from 0.24838
266/266 [==============================] - 311s 1s/step - loss: 0.0188 - mean_absolute_error: 0.1244 - val_loss: 0.0972 - val_mean_absolute_error: 0.3643
Epoch 18/100
266/266 [==============================] - ETA: 0s - loss: 0.0185 - mean_absolute_error: 0.1225
Epoch 18: val_mean_absolute_error did not improve from 0.24838
266/266 [==============================] - 315s 1s/step - loss: 0.0185 - mean_absolute_error: 0.1225 - val_loss: 0.1123 - val_mean_absolute_error: 0.3720
Epoch 19/100
266/266 [==============================] - ETA: 0s - loss: 0.0180 - mean_absolute_error: 0.1201
Epoch 19: val_mean_absolute_error did not improve from 0.24838
266/266 [==============================] - 313s 1s/step - loss: 0.0180 - mean_absolute_error: 0.1201 - val_loss: 0.0892 - val_mean_absolute_error: 0.3352
Epoch 20/100
266/266 [==============================] - ETA: 0s - loss: 0.0174 - mean_absolute_error: 0.1182
Epoch 20: val_mean_absolute_error did not improve from 0.24838
266/266 [==============================] - 315s 1s/step - loss: 0.0174 - mean_absolute_error: 0.1182 - val_loss: 0.2050 - val_mean_absolute_error: 0.5040
Epoch 21/100
266/266 [==============================] - ETA: 0s - loss: 0.0171 - mean_absolute_error: 0.1163
Epoch 21: val_mean_absolute_error did not improve from 0.24838
266/266 [==============================] - 311s 1s/step - loss: 0.0171 - mean_absolute_error: 0.1163 - val_loss: 0.4217 - val_mean_absolute_error: 0.7783
Epoch 22/100
266/266 [==============================] - ETA: 0s - loss: 0.0175 - mean_absolute_error: 0.1153
Epoch 22: val_mean_absolute_error improved from 0.24838 to 0.18463, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 312s 1s/step - loss: 0.0175 - mean_absolute_error: 0.1153 - val_loss: 0.0327 - val_mean_absolute_error: 0.1846
Epoch 23/100
266/266 [==============================] - ETA: 0s - loss: 0.0198 - mean_absolute_error: 0.1162
Epoch 23: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 315s 1s/step - loss: 0.0198 - mean_absolute_error: 0.1162 - val_loss: 0.1018 - val_mean_absolute_error: 0.3389
Epoch 24/100
266/266 [==============================] - ETA: 0s - loss: 0.0175 - mean_absolute_error: 0.1132
Epoch 24: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 311s 1s/step - loss: 0.0175 - mean_absolute_error: 0.1132 - val_loss: 0.0420 - val_mean_absolute_error: 0.2083
Epoch 25/100
266/266 [==============================] - ETA: 0s - loss: 0.0160 - mean_absolute_error: 0.1115
Epoch 25: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 312s 1s/step - loss: 0.0160 - mean_absolute_error: 0.1115 - val_loss: 0.0534 - val_mean_absolute_error: 0.2438
Epoch 26/100
266/266 [==============================] - ETA: 0s - loss: 0.0159 - mean_absolute_error: 0.1113
Epoch 26: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 312s 1s/step - loss: 0.0159 - mean_absolute_error: 0.1113 - val_loss: 0.3036 - val_mean_absolute_error: 0.6249
Epoch 27/100
266/266 [==============================] - ETA: 0s - loss: 0.0152 - mean_absolute_error: 0.1093
Epoch 27: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 312s 1s/step - loss: 0.0152 - mean_absolute_error: 0.1093 - val_loss: 0.1798 - val_mean_absolute_error: 0.4255
Epoch 28/100
266/266 [==============================] - ETA: 0s - loss: 0.0147 - mean_absolute_error: 0.1080
Epoch 28: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 310s 1s/step - loss: 0.0147 - mean_absolute_error: 0.1080 - val_loss: 0.6203 - val_mean_absolute_error: 1.0310
Epoch 29/100
266/266 [==============================] - ETA: 0s - loss: 0.0148 - mean_absolute_error: 0.1073
Epoch 29: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 312s 1s/step - loss: 0.0148 - mean_absolute_error: 0.1073 - val_loss: 0.1298 - val_mean_absolute_error: 0.4055
Epoch 30/100
266/266 [==============================] - ETA: 0s - loss: 0.0147 - mean_absolute_error: 0.1068
Epoch 30: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 313s 1s/step - loss: 0.0147 - mean_absolute_error: 0.1068 - val_loss: 0.1385 - val_mean_absolute_error: 0.4151
Epoch 31/100
266/266 [==============================] - ETA: 0s - loss: 0.0156 - mean_absolute_error: 0.1079
Epoch 31: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 310s 1s/step - loss: 0.0156 - mean_absolute_error: 0.1079 - val_loss: 0.0342 - val_mean_absolute_error: 0.1951
Epoch 32/100
266/266 [==============================] - ETA: 0s - loss: 0.0142 - mean_absolute_error: 0.1056
Epoch 32: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 312s 1s/step - loss: 0.0142 - mean_absolute_error: 0.1056 - val_loss: 0.1390 - val_mean_absolute_error: 0.4677
Epoch 33/100
266/266 [==============================] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.1051
Epoch 33: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 312s 1s/step - loss: 0.0141 - mean_absolute_error: 0.1051 - val_loss: 0.1062 - val_mean_absolute_error: 0.3594
Epoch 34/100
266/266 [==============================] - ETA: 0s - loss: 0.0139 - mean_absolute_error: 0.1037
Epoch 34: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 311s 1s/step - loss: 0.0139 - mean_absolute_error: 0.1037 - val_loss: 0.1157 - val_mean_absolute_error: 0.4164
Epoch 35/100
266/266 [==============================] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.1036
Epoch 35: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 313s 1s/step - loss: 0.0141 - mean_absolute_error: 0.1036 - val_loss: 0.1173 - val_mean_absolute_error: 0.3836
Epoch 36/100
266/266 [==============================] - ETA: 0s - loss: 0.0184 - mean_absolute_error: 0.1057
Epoch 36: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 315s 1s/step - loss: 0.0184 - mean_absolute_error: 0.1057 - val_loss: 0.0881 - val_mean_absolute_error: 0.3357
Epoch 37/100
266/266 [==============================] - ETA: 0s - loss: 0.0139 - mean_absolute_error: 0.1027
Epoch 37: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 311s 1s/step - loss: 0.0139 - mean_absolute_error: 0.1027 - val_loss: 0.0412 - val_mean_absolute_error: 0.1994
Epoch 38/100
266/266 [==============================] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.1009
Epoch 38: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 312s 1s/step - loss: 0.0129 - mean_absolute_error: 0.1009 - val_loss: 0.0765 - val_mean_absolute_error: 0.3024
Epoch 39/100
266/266 [==============================] - ETA: 0s - loss: 0.0141 - mean_absolute_error: 0.1013
Epoch 39: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 315s 1s/step - loss: 0.0141 - mean_absolute_error: 0.1013 - val_loss: 0.3386 - val_mean_absolute_error: 0.6643
Epoch 40/100
266/266 [==============================] - ETA: 0s - loss: 0.0133 - mean_absolute_error: 0.1006
Epoch 40: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 311s 1s/step - loss: 0.0133 - mean_absolute_error: 0.1006 - val_loss: 0.0414 - val_mean_absolute_error: 0.2173
Epoch 41/100
266/266 [==============================] - ETA: 0s - loss: 0.0125 - mean_absolute_error: 0.0989
Epoch 41: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 313s 1s/step - loss: 0.0125 - mean_absolute_error: 0.0989 - val_loss: 0.0401 - val_mean_absolute_error: 0.1976
Epoch 42/100
266/266 [==============================] - ETA: 0s - loss: 0.0126 - mean_absolute_error: 0.0983
Epoch 42: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 316s 1s/step - loss: 0.0126 - mean_absolute_error: 0.0983 - val_loss: 0.0397 - val_mean_absolute_error: 0.2045
Epoch 43/100
266/266 [==============================] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0979
Epoch 43: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 311s 1s/step - loss: 0.0124 - mean_absolute_error: 0.0979 - val_loss: 0.1439 - val_mean_absolute_error: 0.3492
Epoch 44/100
266/266 [==============================] - ETA: 0s - loss: 0.0136 - mean_absolute_error: 0.0992
Epoch 44: val_mean_absolute_error did not improve from 0.18463
266/266 [==============================] - 316s 1s/step - loss: 0.0136 - mean_absolute_error: 0.0992 - val_loss: 0.1450 - val_mean_absolute_error: 0.4073
Epoch 45/100
266/266 [==============================] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0972
Epoch 45: val_mean_absolute_error improved from 0.18463 to 0.16060, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 310s 1s/step - loss: 0.0124 - mean_absolute_error: 0.0972 - val_loss: 0.0263 - val_mean_absolute_error: 0.1606
Epoch 46/100
266/266 [==============================] - ETA: 0s - loss: 0.0119 - mean_absolute_error: 0.0963
Epoch 46: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 311s 1s/step - loss: 0.0119 - mean_absolute_error: 0.0963 - val_loss: 0.0328 - val_mean_absolute_error: 0.1838
Epoch 47/100
266/266 [==============================] - ETA: 0s - loss: 0.0117 - mean_absolute_error: 0.0962
Epoch 47: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 312s 1s/step - loss: 0.0117 - mean_absolute_error: 0.0962 - val_loss: 0.1885 - val_mean_absolute_error: 0.4845
Epoch 48/100
266/266 [==============================] - ETA: 0s - loss: 0.0121 - mean_absolute_error: 0.0962
Epoch 48: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 316s 1s/step - loss: 0.0121 - mean_absolute_error: 0.0962 - val_loss: 0.0841 - val_mean_absolute_error: 0.2983
Epoch 49/100
266/266 [==============================] - ETA: 0s - loss: 0.0124 - mean_absolute_error: 0.0966
Epoch 49: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 311s 1s/step - loss: 0.0124 - mean_absolute_error: 0.0966 - val_loss: 0.1809 - val_mean_absolute_error: 0.4109
Epoch 50/100
266/266 [==============================] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0948
Epoch 50: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 314s 1s/step - loss: 0.0115 - mean_absolute_error: 0.0948 - val_loss: 0.0881 - val_mean_absolute_error: 0.3203
Epoch 51/100
266/266 [==============================] - ETA: 0s - loss: 0.0114 - mean_absolute_error: 0.0939
Epoch 51: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 313s 1s/step - loss: 0.0114 - mean_absolute_error: 0.0939 - val_loss: 0.0806 - val_mean_absolute_error: 0.3241
Epoch 52/100
266/266 [==============================] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0943
Epoch 52: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 310s 1s/step - loss: 0.0115 - mean_absolute_error: 0.0943 - val_loss: 0.3496 - val_mean_absolute_error: 0.7069
Epoch 53/100
266/266 [==============================] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0928
Epoch 53: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 313s 1s/step - loss: 0.0111 - mean_absolute_error: 0.0928 - val_loss: 0.0472 - val_mean_absolute_error: 0.2395
Epoch 54/100
266/266 [==============================] - ETA: 0s - loss: 0.0111 - mean_absolute_error: 0.0932
Epoch 54: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 316s 1s/step - loss: 0.0111 - mean_absolute_error: 0.0932 - val_loss: 0.0354 - val_mean_absolute_error: 0.2006
Epoch 55/100
266/266 [==============================] - ETA: 0s - loss: 0.0112 - mean_absolute_error: 0.0936
Epoch 55: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 311s 1s/step - loss: 0.0112 - mean_absolute_error: 0.0936 - val_loss: 0.2803 - val_mean_absolute_error: 0.6176
Epoch 56/100
266/266 [==============================] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.0934
Epoch 56: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 312s 1s/step - loss: 0.0115 - mean_absolute_error: 0.0934 - val_loss: 0.0847 - val_mean_absolute_error: 0.3040
Epoch 57/100
266/266 [==============================] - ETA: 0s - loss: 0.0109 - mean_absolute_error: 0.0926
Epoch 57: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0109 - mean_absolute_error: 0.0926 - val_loss: 0.0259 - val_mean_absolute_error: 0.1659
Epoch 58/100
266/266 [==============================] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0920
Epoch 58: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 310s 1s/step - loss: 0.0105 - mean_absolute_error: 0.0920 - val_loss: 0.2440 - val_mean_absolute_error: 0.5937
Epoch 59/100
266/266 [==============================] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0917
Epoch 59: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 312s 1s/step - loss: 0.0107 - mean_absolute_error: 0.0917 - val_loss: 0.0572 - val_mean_absolute_error: 0.2368
Epoch 60/100
266/266 [==============================] - ETA: 0s - loss: 0.0107 - mean_absolute_error: 0.0925
Epoch 60: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 313s 1s/step - loss: 0.0107 - mean_absolute_error: 0.0925 - val_loss: 0.0768 - val_mean_absolute_error: 0.3067
Epoch 61/100
266/266 [==============================] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0907
Epoch 61: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 310s 1s/step - loss: 0.0102 - mean_absolute_error: 0.0907 - val_loss: 0.1936 - val_mean_absolute_error: 0.5566
Epoch 62/100
266/266 [==============================] - ETA: 0s - loss: 0.0102 - mean_absolute_error: 0.0901
Epoch 62: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 312s 1s/step - loss: 0.0102 - mean_absolute_error: 0.0901 - val_loss: 0.1850 - val_mean_absolute_error: 0.4654
Epoch 63/100
266/266 [==============================] - ETA: 0s - loss: 0.0105 - mean_absolute_error: 0.0907
Epoch 63: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0105 - mean_absolute_error: 0.0907 - val_loss: 0.1212 - val_mean_absolute_error: 0.3947
Epoch 64/100
266/266 [==============================] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0904
Epoch 64: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 310s 1s/step - loss: 0.0101 - mean_absolute_error: 0.0904 - val_loss: 0.1528 - val_mean_absolute_error: 0.4287
Epoch 65/100
266/266 [==============================] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0913
Epoch 65: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 313s 1s/step - loss: 0.0103 - mean_absolute_error: 0.0913 - val_loss: 0.1720 - val_mean_absolute_error: 0.5032
Epoch 66/100
266/266 [==============================] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0908
Epoch 66: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 314s 1s/step - loss: 0.0103 - mean_absolute_error: 0.0908 - val_loss: 0.0319 - val_mean_absolute_error: 0.1808
Epoch 67/100
266/266 [==============================] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0905
Epoch 67: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 311s 1s/step - loss: 0.0103 - mean_absolute_error: 0.0905 - val_loss: 0.0944 - val_mean_absolute_error: 0.2813
Epoch 68/100
266/266 [==============================] - ETA: 0s - loss: 0.0104 - mean_absolute_error: 0.0901
Epoch 68: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 312s 1s/step - loss: 0.0104 - mean_absolute_error: 0.0901 - val_loss: 0.0962 - val_mean_absolute_error: 0.3331
Epoch 69/100
266/266 [==============================] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0896
Epoch 69: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 310s 1s/step - loss: 0.0101 - mean_absolute_error: 0.0896 - val_loss: 0.0285 - val_mean_absolute_error: 0.1760
Epoch 70/100
266/266 [==============================] - ETA: 0s - loss: 0.0099 - mean_absolute_error: 0.0896
Epoch 70: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 311s 1s/step - loss: 0.0099 - mean_absolute_error: 0.0896 - val_loss: 0.1943 - val_mean_absolute_error: 0.5017
Epoch 71/100
266/266 [==============================] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.0891
Epoch 71: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 313s 1s/step - loss: 0.0101 - mean_absolute_error: 0.0891 - val_loss: 0.1364 - val_mean_absolute_error: 0.3892
Epoch 72/100
266/266 [==============================] - ETA: 0s - loss: 0.0097 - mean_absolute_error: 0.0888
Epoch 72: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0097 - mean_absolute_error: 0.0888 - val_loss: 0.1458 - val_mean_absolute_error: 0.3791
Epoch 73/100
266/266 [==============================] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0883
Epoch 73: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 311s 1s/step - loss: 0.0096 - mean_absolute_error: 0.0883 - val_loss: 0.1681 - val_mean_absolute_error: 0.4888
Epoch 74/100
266/266 [==============================] - ETA: 0s - loss: 0.0096 - mean_absolute_error: 0.0886
Epoch 74: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 312s 1s/step - loss: 0.0096 - mean_absolute_error: 0.0886 - val_loss: 0.0278 - val_mean_absolute_error: 0.1701
Epoch 75/100
266/266 [==============================] - ETA: 0s - loss: 0.0093 - mean_absolute_error: 0.0868
Epoch 75: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 310s 1s/step - loss: 0.0093 - mean_absolute_error: 0.0868 - val_loss: 0.0470 - val_mean_absolute_error: 0.2454
Epoch 76/100
266/266 [==============================] - ETA: 0s - loss: 0.0094 - mean_absolute_error: 0.0873
Epoch 76: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 311s 1s/step - loss: 0.0094 - mean_absolute_error: 0.0873 - val_loss: 0.0904 - val_mean_absolute_error: 0.3290
Epoch 77/100
266/266 [==============================] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0857
Epoch 77: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 313s 1s/step - loss: 0.0091 - mean_absolute_error: 0.0857 - val_loss: 0.0441 - val_mean_absolute_error: 0.2364
Epoch 78/100
266/266 [==============================] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0863
Epoch 78: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 324s 1s/step - loss: 0.0092 - mean_absolute_error: 0.0863 - val_loss: 0.0856 - val_mean_absolute_error: 0.3365
Epoch 79/100
266/266 [==============================] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0855
Epoch 79: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 317s 1s/step - loss: 0.0091 - mean_absolute_error: 0.0855 - val_loss: 0.1017 - val_mean_absolute_error: 0.3647
Epoch 80/100
266/266 [==============================] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0854
Epoch 80: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 319s 1s/step - loss: 0.0090 - mean_absolute_error: 0.0854 - val_loss: 0.1716 - val_mean_absolute_error: 0.4971
Epoch 81/100
266/266 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0848
Epoch 81: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 319s 1s/step - loss: 0.0088 - mean_absolute_error: 0.0848 - val_loss: 0.0884 - val_mean_absolute_error: 0.3288
Epoch 82/100
266/266 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0847
Epoch 82: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 316s 1s/step - loss: 0.0088 - mean_absolute_error: 0.0847 - val_loss: 0.0719 - val_mean_absolute_error: 0.2930
Epoch 83/100
266/266 [==============================] - ETA: 0s - loss: 0.0090 - mean_absolute_error: 0.0852
Epoch 83: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0090 - mean_absolute_error: 0.0852 - val_loss: 0.0436 - val_mean_absolute_error: 0.2193
Epoch 84/100
266/266 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0848
Epoch 84: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 320s 1s/step - loss: 0.0088 - mean_absolute_error: 0.0848 - val_loss: 0.2361 - val_mean_absolute_error: 0.5598
Epoch 85/100
266/266 [==============================] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0844
Epoch 85: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 316s 1s/step - loss: 0.0087 - mean_absolute_error: 0.0844 - val_loss: 0.0690 - val_mean_absolute_error: 0.2702
Epoch 86/100
266/266 [==============================] - ETA: 0s - loss: 0.0088 - mean_absolute_error: 0.0844
Epoch 86: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 318s 1s/step - loss: 0.0088 - mean_absolute_error: 0.0844 - val_loss: 0.0406 - val_mean_absolute_error: 0.2288
Epoch 87/100
266/266 [==============================] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0828
Epoch 87: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 322s 1s/step - loss: 0.0085 - mean_absolute_error: 0.0828 - val_loss: 0.0626 - val_mean_absolute_error: 0.2852
Epoch 88/100
266/266 [==============================] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0830
Epoch 88: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0084 - mean_absolute_error: 0.0830 - val_loss: 0.0248 - val_mean_absolute_error: 0.1609
Epoch 89/100
266/266 [==============================] - ETA: 0s - loss: 0.0083 - mean_absolute_error: 0.0824
Epoch 89: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0083 - mean_absolute_error: 0.0824 - val_loss: 0.0330 - val_mean_absolute_error: 0.1934
Epoch 90/100
266/266 [==============================] - ETA: 0s - loss: 0.0086 - mean_absolute_error: 0.0834
Epoch 90: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0086 - mean_absolute_error: 0.0834 - val_loss: 0.1072 - val_mean_absolute_error: 0.4011
Epoch 91/100
266/266 [==============================] - ETA: 0s - loss: 0.0085 - mean_absolute_error: 0.0829
Epoch 91: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 316s 1s/step - loss: 0.0085 - mean_absolute_error: 0.0829 - val_loss: 0.1402 - val_mean_absolute_error: 0.3879
Epoch 92/100
266/266 [==============================] - ETA: 0s - loss: 0.0084 - mean_absolute_error: 0.0821
Epoch 92: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 319s 1s/step - loss: 0.0084 - mean_absolute_error: 0.0821 - val_loss: 0.0543 - val_mean_absolute_error: 0.2511
Epoch 93/100
266/266 [==============================] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0807
Epoch 93: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 323s 1s/step - loss: 0.0080 - mean_absolute_error: 0.0807 - val_loss: 0.0417 - val_mean_absolute_error: 0.2167
Epoch 94/100
266/266 [==============================] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0823
Epoch 94: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 316s 1s/step - loss: 0.0082 - mean_absolute_error: 0.0823 - val_loss: 0.0359 - val_mean_absolute_error: 0.1991
Epoch 95/100
266/266 [==============================] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0813
Epoch 95: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 315s 1s/step - loss: 0.0082 - mean_absolute_error: 0.0813 - val_loss: 0.0627 - val_mean_absolute_error: 0.2611
Epoch 96/100
266/266 [==============================] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0812
Epoch 96: val_mean_absolute_error did not improve from 0.16060
266/266 [==============================] - 322s 1s/step - loss: 0.0080 - mean_absolute_error: 0.0812 - val_loss: 0.0828 - val_mean_absolute_error: 0.3491
Epoch 97/100
266/266 [==============================] - ETA: 0s - loss: 0.0080 - mean_absolute_error: 0.0806
Epoch 97: val_mean_absolute_error improved from 0.16060 to 0.13640, saving model to Models\Simple-4-(128, 128).h5
266/266 [==============================] - 315s 1s/step - loss: 0.0080 - mean_absolute_error: 0.0806 - val_loss: 0.0191 - val_mean_absolute_error: 0.1364
Epoch 98/100
266/266 [==============================] - ETA: 0s - loss: 0.0076 - mean_absolute_error: 0.0791
Epoch 98: val_mean_absolute_error did not improve from 0.13640
266/266 [==============================] - 318s 1s/step - loss: 0.0076 - mean_absolute_error: 0.0791 - val_loss: 0.0403 - val_mean_absolute_error: 0.2137
Epoch 99/100
266/266 [==============================] - ETA: 0s - loss: 0.0079 - mean_absolute_error: 0.0803
Epoch 99: val_mean_absolute_error did not improve from 0.13640
266/266 [==============================] - 320s 1s/step - loss: 0.0079 - mean_absolute_error: 0.0803 - val_loss: 0.0952 - val_mean_absolute_error: 0.3241
Epoch 100/100
266/266 [==============================] - ETA: 0s - loss: 0.0076 - mean_absolute_error: 0.0791
Epoch 100: val_mean_absolute_error did not improve from 0.13640
266/266 [==============================] - 317s 1s/step - loss: 0.0076 - mean_absolute_error: 0.0791 - val_loss: 0.0245 - val_mean_absolute_error: 0.1580

For pog corrected train3.csv mse_loss: 0.0082
For pog corrected validation3.csv mse_loss: 0.0331
For pog corrected test3.csv mse_loss: 0.0310

